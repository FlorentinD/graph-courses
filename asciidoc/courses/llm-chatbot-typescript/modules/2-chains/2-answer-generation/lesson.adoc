= Answer Generation Chain
:type: challenge
:lab: {repository-blob}/main/src/modules/agent/
:repository-raw: /Users/adam/graphacademy/llm-chatbot-typescript
// :repository-include: {repository-raw}/main
:repository-include: {repository-raw}
:test-filename: answer-generation.chain.test.ts

When implementing **Retreival Augmented Generation (RAG)**, you provide information as part of the prompt that will help the LLM to generate an accurate response.

To complete this challenge, you must create a function that will initiate a new chain.

The function must:

1. Accept a `BaseLanguageModel` as a parameter and return a `RunnableSequence`.
2. Create a new prompt template that instructs the LLM to respond to a question based solely on the context provided.  The prompt must have placeholders for a **question** and **context**.
3. The prompt should be passed to an LLM.
4. The output from the LLM should be parsed as a string.

== 1. Create a new function

Inside the `src/modules/agent/chains/` folder, create a new file called `answer-generation.chain.ts`.

Create a new interface called `GenerateAnswerInput`, which will be used to assert that the input of the function will must contain an object with keys for `question` and `context` as strings.


[source,typescript]
----
include::{repository-include}/src/solutions/modules/agent/chains/answer-generation.chain.ts[tag=interface]
----

Next, create a new function called `initGenerateAnswerChain`.
The function should be exported as `default`.

The function should accept one parameter, an instance that extends `BaseLanguageModel`.

The output should be a `RunnableSequence` with generics to define the input (`GenerateAnswerInput`) and the output (a `string`).

[source,typescript]
----
include::{repository-include}/src/solutions/modules/agent/chains/answer-generation.chain.ts[tag=signature]
----

Inside the file, create a new function with the following signature.



== 2. Create a Prompt Template


Inside the function, use the `PromptTemplate.fromTemplate()` method to create a new prompt template.
Use the following prompt as the first parameter.

.Prompt
[source]
----
include::{repository-include}/prompts/answer-generation.txt[]
----

== 3. Create the RunnableSequence

Use the `RunnableSequence.from()` method to create a new chain.
The chain should pass the prompt to the llm passed as a parameter, then format the response as a string using a new instance of the `StringOutputParser`.

[source,typescript]
----
include::{repository-include}/src/solutions/modules/agent/chains/answer-generation.chain.ts[tag=sequence]
----


== Working Solution

.Click here to reveal the fully-implemented `answer-generation.chain.ts`
[%collapsible]
====
[source,js,indent=0]
----
include::{repository-include}/src/solutions/modules/agent/chains/answer-generation.chain.ts[]
----
====



include::../../../includes/test.adoc[]


read::it works![]


[.summary]
== Summary

In this lesson, you implemented your first chain using LCEL.
You will use this in future lessons you will use this chain to convert raw data from Neo4j into a natural language response.

In the next lesson, you will learn how to validate the response generated by the LLM.
