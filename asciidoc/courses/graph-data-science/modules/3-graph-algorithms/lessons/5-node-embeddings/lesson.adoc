= Node Embeddings
:type: quiz


[.transcript]
== Introduction

The goal of node embedding is to compute low-dimensional vector representations of nodes such that similarity between vectors (eg. dot product) approximates similarity between nodes in the original graph. These vectors, also called embeddings, can be extremely useful for exploratory data analysis, similarity measurements, and machine learning.



=== Intuition
The below diagram illustrates the concept behind node embedding, whereby nodes that are close together in the graph end up being close together in the 2-dimensional embedding space.  The embedding thus took the structure from the graph, the n-dimensional adjacency matrix, and approximated it in 2-dimensional vectors for each node.  The embedding vectors are much more efficient to use for downstream process due to significantly reduced dimensionality.  They could be used for cluster analysis for example, or as features to train a node classification or link prediction model.

image::images/node-embeddings-1.png[title="Concept Behind Node Embeddings",600]

Of course, in real-world problems node embeddings will usually be larger than 2 dimensions, often ending up in the hundreds or larger, especially when applied to bigger graphs with millions or billions of nodes. Similarity in the graph also doesn't have to be based strictly on locality.  While similarity based on distance in relationship hops and common neighbors is perhaps most common in application, node embedding can also consider node properties and other "global-view" node attributes when calculating embedding vectors.



=== Use Cases
Node embeddings have applications across multiple use cases, from recommendations, to anomaly and fraud detection, entity resolution and other forms of knowledge graph completion.

Node embeddings don't offer insights by themselves, they are created to enable or scale other analytics.  Common workflows include:

* *Exploratory Data Analysis (EDA)* such as visualizing the embeddings in a TSNE plot to better understand the graph structure and potential clusters of nodes

* *Similarity Measurements*: Node Embeddings allow you to scale similarity inferences in large graphs using K Nearest Neighbor (KNN) or other techniques.  This can be useful for scaling memory based recommendation systems, such as variations of collaborative filtering.  It can also be used for semi-supervised techniques in areas like fraud detection, where we may want to generate leads that are similar to a group of known fraudulent entities for example.

* *Features for Machine Learning*: Node embeddings naturally plug in as features for a variety of machine learning problems. For example, in a graph of user purchases for on online retailer, we could use graph embeddings to train a machine learning to predict what products a user may be interested in buying next.


=== FastRP

GDS offers a custom implementation of a node embedding technique called Fast Random Projection, or FastRP for short.  FastRP leverages probabilistic sampling techniques to generate sparse representations of the graph allowing for _extremely_ fast calculation of node embeddings that are comparative in quality to those produced with traditional random walk and neural net techniques such as Node2vec and GraphSage. This makes FastRP a great choice for getting started with exploring embeddings on your graph in GDS.


//we will need to add code examples....this is tricky for recommendations so we may just want to determine similar actors in the graph.....that would be faster.
Below is an example of generating embeddings on our movies recommendations graph.


In this example I am just projecting the user ratings portion of the graph. We will project the RATED relationship with an UNDIRECTED orientation as this allows FastRP to traverse the graph in both directions, which is usually the behavior we want when applying FastRP. Running FastRP on the part of the graph will give us

Then we can run fastRP like so:

These embeddings, can, in theory, be used for similarity measurements or applied in machine learning to provide personalized recommendations to users. In a real world use case we would want to either filter the projected graph to just higher than average rating values or use a shifted version of the rating value as weights to more accuractly take into account the ratings.


=== Other Node Embedding Algorithms
GD has also implemented Node2Vec and GraphSage node embeddings.  You can read more about

== Check your understanding


[.summary]
== Summary
